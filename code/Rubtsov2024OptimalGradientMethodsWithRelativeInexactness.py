# -*- coding: utf-8 -*-
"""PEP_ISTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dnsw8b1gEeOtzqDupMgrSZg8qmSVopF_

# Installing libraries
"""

pip install PEPit

!pip install Mosek

from PEPit import PEP
from PEPit.functions import SmoothConvexFunction, SmoothStronglyConvexFunction
from PEPit.primitive_steps import inexact_gradient_step
from PEPit.point import Point
import mosek

import cvxpy as cp

print(cp.installed_solvers())

"""If solver MOSEK is not available one can use SCS solver instead"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import scipy
import re
from numpy.linalg import norm
from math import floor
from tqdm import tqdm
from itertools import product
from sklearn.model_selection import train_test_split

#graphs settings
plt.rcParams['lines.linewidth'] = 1.5
plt.rcParams['lines.markersize'] = 12
plt.rcParams['xtick.labelsize'] = 20
plt.rcParams['ytick.labelsize'] = 20
plt.rcParams['legend.fontsize'] = 14
plt.rcParams['axes.titlesize'] = 15
plt.rcParams['axes.labelsize'] = 15

"""# CODE"""

#Obtain inexact gradient with both relative and absolute setup
def inexact_grad(x, f, epsilon, notion = 'relative'):
    exact_grad_x = f.gradient(x)
    inexact_grad_x = Point()
    if epsilon != 0.:
        if notion == 'absolute':
            f.add_constraint((exact_grad_x - inexact_grad_x) ** 2 - epsilon ** 2 <= 0)
        elif notion == 'relative':
            f.add_constraint((exact_grad_x - inexact_grad_x) ** 2 - epsilon ** 2 * (exact_grad_x ** 2) <= 0)
        else:
            raise ValueError("inexact_gradient_step supports only notion in ['absolute', 'relative'],"
                            " got {}".format(notion))
        return inexact_grad_x, exact_grad_x
    else:
        return exact_grad_x, exact_grad_x

#Implementation of ISTM algorithm on whole R^2
def ISTM(f, N, x_0, L, a = None, epsilon = 0., p = 2, notion = 'relative'):

    if a is None:
        a = 2
    x_k = x_0
    alpha_k = 0
    A_k = 0
    y_k = x_k
    z_k = x_k

    for k in range(N):
        alpha_k_plus_1  = (k + 2) ** (p - 1) / (2 * a * L)
        A_k_plus_1 = alpha_k_plus_1 + A_k

        x_k_plus_1 = 1/A_k_plus_1 * (A_k * y_k + alpha_k_plus_1 * z_k)
        inexact_grad_x, exact_grad_x = inexact_grad(x_k_plus_1 , f, epsilon, notion = notion )
        z_k_plus_1 = z_k - alpha_k_plus_1 * inexact_grad_x
        y_k_plus_1 = 1 / A_k_plus_1 * (A_k * y_k + alpha_k_plus_1 * z_k_plus_1)

        y_k = y_k_plus_1
        z_k = z_k_plus_1
        x_k = x_k_plus_1
        A_k = A_k_plus_1
        alpha_k = alpha_k_plus_1

    return y_k

def solve_problem( N, epsilon, p, a = None, L = 1, mu = None, problem_algo = ISTM, verbose = 0, notion = 'relative', solver = 'MOSEK'):
    problem = PEP()

    # Declare a (strongly) convex smooth function
    if mu is None or mu == 0:
        func = problem.declare_function(SmoothConvexFunction, L = L)
    else:
        func = problem.declare_function(SmoothStronglyConvexFunction, L = L, mu = mu)

    # Start by defining its unique optimal point xs = x_* and corresponding function value fs = f_*
    x_s = func.stationary_point()
    f_s = func(x_s)

    # Then define the starting point x0 of the algorithm
    # as well as corresponding inexact gradient and function value g0 and f0
    x_0 = problem.set_initial_point()

    # Set the initial constraint that is the distance between f0 and f_*

    problem.set_initial_condition((x_0 - x_s) ** 2 <= 1)
    #calculate iterations of intermediate_Gradient


    y_N = problem_algo(func, N, x_0, L = L, a = a, epsilon = epsilon, p = p, notion = notion)

    problem.set_performance_metric(func(y_N) - f_s)

    #Solve optimization problem

    pepit_verbose = max(verbose, 0)
    pepit_tau = problem.solve(verbose = pepit_verbose, solver = solver)
    return pepit_tau

# run iter_num of iterations of algorithm and return list of pepit_taus
def run_iterations(iter_num, epsilon, p = 2, a_s = None, L = 1, mu = None, problem_algo = ISTM, verbose = 0, notion = 'relative', solver = 'MOSEK'):
    pepit_taus = []
    if verbose == 0:
        internal_range = range(iter_num + 1)
    else:
        internal_range = tqdm(range(iter_num + 1))
    if a_s is None:
        a_s = [None] * (iter_num + 1)

    for i, N in enumerate(internal_range):
        pepit_taus.append(solve_problem(N = N, epsilon = epsilon, a = a_s[i], L = L, mu = mu, p = p, problem_algo = problem_algo, verbose = verbose, notion = notion, solver = solver))

    return pepit_taus

"""# Experiment

For a more detailed description of the experimental conditions, we refer the reader to the original article

## STM N disconvergence (Extra Exp 1)
"""

first_exp_data_dict = dict()

a_range = list(range(2,30,1))
eps_range = [0.65, 0.8, 0.90]
p = 2
mu = 0
L  = 1
iter_num = 15

for eps in tqdm(eps_range):
    a_dict = dict()
    for a in a_range:
        pepit_taus = run_iterations(iter_num, a_s = [a] * (iter_num + 1), epsilon = eps, L = L, mu = mu, p = p)
        a_dict[a] = pepit_taus
    first_exp_data_dict[eps] = a_dict

import pickle

with open('STM_data_dict.pkl', 'wb') as f:
        pickle.dump(first_exp_data_dict, f)

plt.figure(figsize = (12,8))
p = 2
a_range = range(2,30,1)
colors = ['g', 'y', 'b', 'r', 'orange', 'magenta', 'gray', 'yellow']
i = 0
b_range = range(0, 30, 1)

#eps_range = [0.3, 0.5, 0.7, 0.9]
sum_1 = 0
sum_2 = 0
for eps in eps_range:
    #squared_c_dividedby_squared_epsilon = 4.5**2
    N_pep_s = []
    #sum_1 = 0
    #sum_2 = 0

    for a in a_range:
        N_pep = np.argmin(first_exp_data_dict[eps][a])
        if N_pep < iter_num :
            #N_pep_s.append(N_pep)
            #print(N_pep)
            N_pep_s.append(N_pep**p * eps**2)
            sum_1 += N_pep**p * a * eps**2
            sum_2 += a**2
    #if sum_2 != 0:
      #c_new = sum_1/sum_2*eps**2
    #print(c_new**0.5)
    #print(squared_c_dividedby_squared_epsilon)
    #theory = [(squared_c_dividedby_squared_epsilon *a)**(1/p) for a in a_range if (squared_c_dividedby_squared_epsilon *a)**(1/p) < iter_num]
    #theory = [(c_new *a) for a in a_range if (c_new *a) < (iter_num)**2]

    plt.scatter(a_range[:len(N_pep_s)], N_pep_s, color = colors[i], label = 'PEP, $\\hat{\\varepsilon}$ = ' + str(np.round(eps, 2)) + f', p = {p}')
    i += 2

c_new = sum_1/sum_2
print(c_new**0.5)
theory = [(c_new *a) for a in a_range if (c_new *a) < (iter_num)**2]
plt.plot(a_range[:len(theory)], theory, color = colors[i], label = 'theory ' + str(np.round(c_new**0.5, 2)) + '$^2 \\cdot a$, $\\hat{\\varepsilon}$ = ' + str(np.round(eps, 2)) + f', p = {p}')
    #i += 1

plt.xlabel('a', fontsize = 20)
plt.ylabel('$N_{pep}^p \\cdot \\hat{\\varepsilon}^2$', fontsize = 20)
plt.legend()
plt.grid()
#plt.savefig('STM_N_disconvergence.pdf')

"""## STM Convergence (Extra Exp 2)"""

second_exp_data_dict = dict()

eps_range = np.linspace(0.1, 0.95, 7)
p = 2
mu = 0
L  = 1
iter_num = 20

for eps in tqdm(eps_range):
        a_s = [max(1, N**p * eps**2, N**(p / 2) * eps, N**(p / 4) * eps**0.5) for N in range(iter_num + 1)]
        pepit_taus = run_iterations(iter_num, a_s = a_s, epsilon = eps, L = L, mu = mu, p = p)
        second_exp_data_dict[eps] = pepit_taus

import pickle

with open('STM_convergence_dict.pkl', 'wb') as f:
        pickle.dump(second_exp_data_dict, f)

plt.figure(figsize = (12,8))

colors = ['g', 'y', 'b', 'r', 'orange', 'magenta', 'brown']
i = 0
iter_num = 20
eps_range = np.linspace(0.1, 0.95, 7)

for eps in eps_range:
    plt.plot(range(iter_num + 1), second_exp_data_dict[eps], color = colors[i], label = '$\\hat{\\varepsilon}$ = ' + f'{np.round(eps, 3)}')
    i += 1
plt.xlabel('number of iteration N', fontsize = 20)
plt.ylabel('maximum $f(y^N) - f^*$', fontsize = 20)
plt.legend()
plt.grid()
#plt.savefig('STM_Convergence.pdf')

def estimate_coef(x, y):
  # number of observations/points
  n = np.size(x)

  # mean of x and y vector
  m_x = np.mean(x)
  m_y = np.mean(y)
  print(type(x), type(y))
  sum = 0
  for i in range(n):
    sum += x[i] * y[i]

  # calculating cross-deviation and deviation about x
  SS_xy = sum - n*m_y*m_x
  SS_xx = np.sum(x*x) - n*m_x*m_x

  # calculating regression coefficients
  b_1 = SS_xy / SS_xx
  b_0 = m_y - b_1*m_x

  return (b_0, b_1)
