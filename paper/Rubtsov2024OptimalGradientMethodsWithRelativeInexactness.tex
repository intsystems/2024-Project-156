\documentclass{article}
\usepackage{arxiv}
%\documentclass[12pt]{article}
%\usepackage[cp1251]{inputenc}
%\usepackage[russian]{babel}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T2A, T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{hyperref}
\usepackage{amssymb, amsmath, latexsym}
% \usepackage{fullpage}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage{algpseudocode}
\usepackage{tabularx}
\usepackage{paralist}
\usepackage{mathtools}
%\usepackage{tcolorbox}
\usepackage{xcolor}

\usepackage{bbm} %for indicators 1

\usepackage{makecell}
\usepackage{multirow}
\usepackage{booktabs}

\usepackage{nicefrac}  

\usepackage{subcaption}
% \usepackage{subfigure} 
\newcommand{\Exp}{\mathbf{E}}
\newcommand{\Prob}{\mathbf{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\eqdef}{\stackrel{\text{def}}{=}}


\newtheorem{Def}{Определение}[section]


\title{Optimal Gradient Methods with Relative Inexactness}

\author{
	Рубцов Денис \\
	\texttt{rubtsov.dn@phystech.edu} \\
	%% examples of more authors
	\And
	Корнилов Никита \\
	\texttt{kornilov.nm@phystech.edu} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{Optimal Gradient Methods with Relative Inexactness}
\renewcommand{\undertitle}{}
%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Optimal Gradient Methods with Relative Inexactness},
pdfsubject={Методы оптимизации первого порядка},
pdfauthor={Рубцов Д.Н., Корнилов Н.М.},
pdfkeywords={методы первого порядка, ускоренные методы, неточный градиент, относительный шум, Performance Estimation Problem},
}

\begin{document}
\maketitle

\begin{abstract}
Работа посвящена ускоренным методам гладкой выпуклой оптимизации первого порядка с градиентами, известными лишь с некоторой относительной погрешностью. Проведен обзор полученных ранее теоретических результатов об оценках максимально допустимой погрешности, сохраняющей линейную сходимость методов. С помощью анализа численного решения эквивалентной задачи полуопределенного программирования (техника PEP) показана достижимость этих оценок.

\end{abstract}


\keywords{методы первого порядка, ускоренные методы, неточный градиент, относительный шум, Performance Estimation Problem}

\section{\textbf{Введение}}

В данной статье изучаются методы гладкой выпуклой оптимизации первого порядка. Их изучение актуально в связи с высоким успехом их применения во многих приложениях, в том числе в машинном обучении. Во многих ситуациях алгоритмы не имеют доступа к точной информации о градиенте. Так, например, бывает, когда для того, чтобы получить значение градиента, требуется решить другую сложную задачу (например, решить дифференциальное уравнение (подробнее - в \cite{matyukhin2021convex}), не имеющего точного аналитического решения). В данной статье мы сосредоточимся на ситуации, когда градиенты известны с некоторой относительной погрешностью $\hat{\varepsilon} \in [0, 1]$ $$\|\widetilde{\nabla} f(x) - \nabla f(x)\|_2 \leq \hat{\varepsilon} \|\nabla f(x)\|_2$$

Особое место в данной статье отведено технике Performance Estimation Problem (далее -- PEP, см. в \cite{goujaud2022pepit}, \cite{taylor2017smooth}, \cite{taylor2017convex}). С помощью этого инструмента исследования мы продемонстрируем точность полученной ранее в статье \cite{kornilov2023intermediate} теоретической оценки максимальной погрешности, сохраняющей сходимость метода ISTM. Более того, с помощью этой техники мы будем искать оптимальные методы первого порядка, основываясь на принципе жадных алгоритмов (\cite{goujaud2023fundamental}, \cite{kim2018generalizing}). 

Цель данной статьи - найти оптимальные ускоренные методы 

\subsection{\textbf{Определения и обозначения}}


\begin{Def}
\textbf{Скалярное произведение} элементов $x,y \in \R^n$ : $\langle x,y \rangle := \sum\limits_{k=1}^n x_ky_k$.
\end{Def}

\begin{Def}
$\ell_2$-норма элемента $x \in \R^n$ : $\|x\|_2 := \left(\sum\limits_{k=1}^n x_k^2\right)^{1/2}$.
\end{Def}

\begin{Def}
Функция $f$ выпукла, если 
\begin{equation}
    f(y) \geq f(x)+\langle\nabla f(x), y-x\rangle, \quad \forall x, y \in \mathbb{R}^n.
\end{equation}
\end{Def}

\begin{Def}
Функция $f$ - $\mu$-сильно выпукла, если существует константа $\mu > 0$ такая, что:
    \begin{equation}\label{eq:str_cvx}
        f(y) \geq f(x) + \langle \nabla f(x), y - x \rangle + \frac{\mu}{2}\|y - x\|_2^2, \quad x, y \in \R^n. 
    \end{equation}
\end{Def}

\begin{Def}
Функция $f$ --- $L$-гладкая, если существует константа $L > 0$ такая, что:
\begin{equation}\label{smoothness_cond}
    f(y) \leq f(x)+ \left\langle\nabla f(x), y-x\right\rangle + \frac{L}{2} \|y-x\|_2^2, \quad \forall x, y \in \mathbb{R}^n,
\end{equation}
или (эквивалентно) 
\begin{equation}\label{eq_6}
    \|\nabla f(y) - \nabla f(x)\|_2 \leq L \|y - x\|_2, \quad \forall x, y \in \mathbb{R}^n.
\end{equation}
\end{Def}

\begin{Def}
Будем говорить, что мы имеем доступ к градиенту функции $f$ с некоторой относительной погрешностью $\hat{\varepsilon}$, если
\begin{equation}\label{eq_relative_error}
    \|\widetilde{\nabla} f(x) - \nabla f(x)\|_2 \leq \hat{\varepsilon} \|\nabla f(x)\|_2, \quad \forall \hat{\varepsilon} \in [0, 1].
\end{equation}
Здесь $\nabla f(x)$ -- точное значение градиента функции $f$ в точке $x$, а $\widetilde{\nabla} f(x)$ - доступное нам зашумленное значение градиента.
\end{Def}

\begin{Def}
Класс всех $L$-гладких, $\mu$-сильно выпуклых функций будем обозначать $\mathcal{F}_{\mu, L}$. Класс всех $L$-гладких, выпуклых функций будем обозначать $\mathcal{F}_{0, L}$
\end{Def}

\subsection{Intermediate Similar Triangle Method with Relative Noise in Gradient}

В статье \cite{kornilov2023intermediate} рассматривается алгоритм Intermediate Similar Triangle Method. В этой работе была получена наилучшая из известных на данный момент оценок максимальной относительной погрешности, сохраняющей сходимость этого метода в случае сильной выпуклости $\hat{\varepsilon} \lesssim (\mu / L)^{1/2}$. Более того, в выпуклом случае было показано, что при малом числе итераций алгоритм сходится со скоростью $\sim \frac{1}{N^p}$. Однако при числе итераций $N: N^p \hat{\varepsilon}^2 \gtrsim 1$, близость к оптимальному значению функции выходит на плато $\hat{\varepsilon}^2 L R_0^2$, где $R_0 = \|x^0 - x^*\|_2$.  Эти оценки были подтверждены с помощью численных экспериментов, использующих технику PEP, в той же статье. В нашей статье проводятся дополнительные численные эксперименты.

\begin{algorithm}[!ht]
\caption{ Intermediate  Similar Triangle Method (\texttt{ISTM}).}\label{alg_STM_relative}
\begin{algorithmic}[1]
   \REQUIRE Initial point $x^0$, number of iterations $N$, smoothness constant  $L>0$, and step size parameter $a \geq 1$, intermediate parameter $p \in [1,2]$. 
   \STATE Set $A_0 = \alpha_0 = 0, y^0 = z^0 = x^0$.
   \FOR{$k=0,1 ,  \ldots, N-1$}
   \STATE Set $\alpha_{k+1} = \frac{(k+2)^{p-1}}{2aL}, \, A_{k+1} = \alpha_{k+1} + A_k$. \label{item_3}
   \STATE $x^{k+1} = \frac{1}{A_{k+1}} \left(A_k y^k + \alpha_{k+1} z^k\right) $. \label{item_4}
   \STATE $z^{k+1} = z^k - \alpha_{k+1} \widetilde{\nabla} f(x^{k+1})$.
   \STATE $y^{k+1} = \frac{1}{A_{k+1}} \left(A_k y^k + \alpha_{k+1} z^{k+1}\right)$.
   \ENDFOR
    \ENSURE 
	$y^N$.
\end{algorithmic}
\end{algorithm}

\subsection{PEP}
Часто доказательства скорости сходимости методов оптимизации носят неинтуитивный, техничный характер. Они представляют собой цепочку длинных нетривиальных неравенств, оценивающих наихудший случай. Если сразу задаться поиском этого наихудшего случая, то мы придем к бесконечномерной задаче оптимизации на некотором классе функций (например, на $\mathcal{F}_{\mu, L}$). Оказывается, однако, что с помощью плодотворных идей интерполяции (\cite{taylor2017smooth}), такую задачу можно свести к конечномерной задаче полуопределенного программирования (semidefinite programming, далее -- SDP). Эта техника и называется PEP. 

\subsection{О поиске оптимальных методов}
Во всех статьях (в том числе, в \cite{kornilov2023intermediate}) техника PEP применяется для конкретных методов оптимизации первого порядка. Более интересной и важной задачей, которой посвящена данная статья, является поиск оптимального метода первого порядка с зашумленными градиентами. Эта задача является задачей оптимизации на пространстве методов. Мы будем рассматривать лишь определенный класс <<линейных>> методов так, что эта задача превратится в задачу поиска коэффициентов $\alpha, \alpha_0, \alpha_1, ... , \alpha_{k-1}$ линейной комбинации, выражающей $k$-ое приближение точки минимума $x^{k} = \alpha x^0 + \alpha_0 * \widetilde{\nabla} f(x^0) + ... + \alpha_{k-1} *\widetilde{\nabla} f(x^{k - 1})$ через начальную точку итерационного процесса $x^0$ и ответы оракула $\widetilde{\nabla} f(x^i)$. Вообще говоря, $\alpha_i$ зависят от номера итерации $k$ и могут быть найдены жадным алгоритмом на каждой итерации.

\section{Постановка задачи}
Мы решаем задачу безусловной оптимизации \[x^* = \arg \min_{x \in \R^d} f(x)\]
Функция $f(x) \in \mathcal{F}_{\mu, L}$. Мы имеем доступ к оракулу первого порядка, предоставляющему информацию о приближенном значении градиента $\mathcal{O}^{(f)} (x) = \widetilde{\nabla} f(x)$. 


\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}

